
# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„í¬íŠ¸í•©ë‹ˆë‹¤.
import re
import requests
from urllib.parse import urljoin, urlparse, parse_qs
from bs4 import BeautifulSoup
from concurrent.futures import ProcessPoolExecutor, wait, FIRST_COMPLETED
from multiprocessing import Manager
from xss_test import test_xss  # xss_test ëª¨ë“ˆì—ì„œ test_xss í•¨ìˆ˜ë¥¼ ì„í¬íŠ¸í•©ë‹ˆë‹¤.

# ê° í˜ì´ì§€ë¥¼ í¬ë¡¤ë§í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.
def crawl_page(url, visited, collected_urls_params, payloads):
    try:
        # í˜ì´ì§€ì— ìš”ì²­ì„ ë³´ëƒ…ë‹ˆë‹¤.
        headers = {"User-Agent": "Mozilla/5.0"}
        for _ in range(3):  # ì¬ì‹œë„ ë¡œì§ì„ ì¶”ê°€í•©ë‹ˆë‹¤.
            try:
                response = requests.get(url, headers=headers)
                break
            except:
                print(f"ğŸŸ¡ Retry: {url}")
        else:
            print(f"ğŸŸ¡ Failed to fetch: {url}")
            return url, []

        # BeautifulSoup ê°ì²´ë¥¼ ìƒì„±í•˜ì—¬ HTMLì„ íŒŒì‹±í•©ë‹ˆë‹¤.
        soup = BeautifulSoup(response.text, 'html.parser')


        # í˜ì´ì§€ì˜ ëª¨ë“  <script> íƒœê·¸ë¥¼ ì°¾ìŠµë‹ˆë‹¤.
        for script_tag in soup.find_all('script'):
            # <script> íƒœê·¸ì˜ ë‚´ìš©ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
            script_code = script_tag.string
            if script_code:
                # match í•¨ìˆ˜ì—ì„œ = ë¬¸ì ì´ì „ì˜ ë¬¸ìì—´ì„ íŠ¹ìˆ˜ë¬¸ìë¥¼ ì œì™¸í•˜ê³  ë½‘ì•„ì˜µë‹ˆë‹¤.
                match = re.search(r"match\(\[?&\]([\w]+)=?", script_code)
                if match:
                    param_name = match.group(1)
                    # íŒŒë¼ë¯¸í„°ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
                    param_url=f'{urlparse(url)._replace(query="").geturl()}?{param_name}='
                    if param_url not in collected_urls_params:
                        collected_urls_params.append(param_url)
                        print(param_url)
                        test_xss(param_url, payloads)
                
                # match í•¨ìˆ˜ì—ì„œ = ë¬¸ì ì´ì „ì˜ ë¬¸ìì—´ì„ íŠ¹ìˆ˜ë¬¸ìë¥¼ ì œì™¸í•˜ê³  ë½‘ì•„ì˜µë‹ˆë‹¤.
                match = re.search(r'match\(\[?&\]([\w]+)=?', script_code)
                if match:
                    param_name = match.group(1)
                    # íŒŒë¼ë¯¸í„°ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
                    param_url=f'{urlparse(url)._replace(query="").geturl()}?{param_name}='
                    if param_url not in collected_urls_params:
                        collected_urls_params.append(param_url)
                        print(param_url)
                        test_xss(param_url, payloads)
                
                # match í•¨ìˆ˜ì—ì„œ = ë¬¸ì ì´ì „ì˜ ë¬¸ìì—´ì„ íŠ¹ìˆ˜ë¬¸ìë¥¼ ì œì™¸í•˜ê³  ë½‘ì•„ì˜µë‹ˆë‹¤.
                match = re.search(r"indexOf\(\[?&\]([\w]+)=?", script_code)
                if match:
                    param_name = match.group(1)
                    # íŒŒë¼ë¯¸í„°ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
                    param_url=f'{urlparse(url)._replace(query="").geturl()}?{param_name}='
                    if param_url not in collected_urls_params:
                        collected_urls_params.append(param_url)
                        print(param_url)
                        test_xss(param_url, payloads)
                        
                # match í•¨ìˆ˜ì—ì„œ = ë¬¸ì ì´ì „ì˜ ë¬¸ìì—´ì„ íŠ¹ìˆ˜ë¬¸ìë¥¼ ì œì™¸í•˜ê³  ë½‘ì•„ì˜µë‹ˆë‹¤.
                match = re.search(r'indexOf\(\[?&\]([\w]+)=?', script_code)
                if match:
                    param_name = match.group(1)
                    # íŒŒë¼ë¯¸í„°ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
                    param_url=f'{urlparse(url)._replace(query="").geturl()}?{param_name}='
                    if param_url not in collected_urls_params:
                        collected_urls_params.append(param_url)
                        print(param_url)
                        test_xss(param_url, payloads)

                #get í•¨ìˆ˜ì—ì„œ ì‹±ê¸€ ì¿¼í…Œì´ì…˜ì´ ìˆë“  ì—†ë“  íŒŒë¼ë¯¸í„°ë¥¼ ë½‘ì•„ì˜µë‹ˆë‹¤.
                match = re.search(r"\.get\(\s*'?(.*?)'?\s*\);", script_code)
                if match:
                    param_name = match.group(1)
                    #íŒŒë¼ë¯¸í„°ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
                    param_url=f'{urlparse(url)._replace(query="").geturl()}?{param_name}='
                    if param_url not in collected_urls_params:
                        collected_urls_params.append(param_url)
                        print(param_url)
                        test_xss(param_url, payloads)
                
                match = re.search(r"location\.href\.slice", script_code)
                if match:
                    param_url=f'{urlparse(url)._replace(query="").geturl()}#'
                    if param_url not in collected_urls_params:
                        collected_urls_params.append(param_url)
                        print(param_url)
                        test_xss(param_url, payloads)


                match = re.search(r"location\.hash", script_code)
                if match:
                    param_url=f'{urlparse(url)._replace(query="").geturl()}#'
                    if param_url not in collected_urls_params:
                        collected_urls_params.append(param_url)
                        print(param_url)
                        test_xss(param_url, payloads)
                
                match = re.search(r"\#", script_code)
                if match:
                    param_url = f'{urlparse(url)._replace(query="").geturl()}#'
                    if param_url not in collected_urls_params:
                        collected_urls_params.append(param_url)
                        print(param_url)
                        test_xss(param_url, payloads)
                

        # í˜ì´ì§€ ë‚´ì—ì„œ ë°œê²¬í•œ ëª¨ë“  ë§í¬ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
        links = [urljoin(url, link.get('href')) for link in soup.find_all('a') if urljoin(url, link.get('href')) not in visited]
        # í˜„ì¬ í˜ì´ì§€ URLê³¼ ìˆ˜ì§‘í•œ ë§í¬ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        return url, links
        
    except Exception as e:
        # ì˜¤ë¥˜ê°€ ë°œìƒí•œ ê²½ìš°, ì˜¤ë¥˜ ë©”ì‹œì§€ë¥¼ ì¶œë ¥í•˜ê³  í˜„ì¬ í˜ì´ì§€ URLê³¼ ë¹ˆ ë§í¬ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        print(f"ğŸŸ¡ Error occurred: {e}")
        return url, []

# ì›¹ì‚¬ì´íŠ¸ë¥¼ í¬ë¡¤ë§í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.
def crawl_website(start_url, paylaods):
    with Manager() as manager:
        # ë°©ë¬¸í•œ í˜ì´ì§€ì™€ ìˆ˜ì§‘í•œ URLê³¼ íŒŒë¼ë¯¸í„°ì˜ ìŒì„ ì €ì¥í•˜ëŠ” ë¦¬ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        visited = manager.list([start_url])
        collected_urls_params = manager.list()

        # ì‹œì‘ URLì˜ ë„ë©”ì¸ì„ íŒŒì‹±í•©ë‹ˆë‹¤.
        parsed_start_url = urlparse(start_url)
        start_domain = parsed_start_url.netloc

        # ProcessPoolExecutorë¥¼ ì‚¬ìš©í•˜ì—¬ ë³‘ë ¬ë¡œ í˜ì´ì§€ë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤.
        with ProcessPoolExecutor(max_workers=8) as executor:
            futures = {executor.submit(crawl_page, start_url, visited, collected_urls_params, paylaods)}
            while futures:
                # ì™„ë£Œëœ ì‘ì—…ì„ ê¸°ë‹¤ë¦½ë‹ˆë‹¤.
                done, futures = wait(futures, return_when=FIRST_COMPLETED)
                for future in done:
                    _, links = future.result()
                    for link in links:
                        # ë§í¬ê°€ ì•„ì§ ë°©ë¬¸í•˜ì§€ ì•Šì€ ë§í¬ì´ê³  ê°™ì€ ë„ë©”ì¸ì— ì†í•˜ëŠ” ê²½ìš°, ê·¸ ë§í¬ë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤.
                        if link not in visited and urlparse(link).netloc == start_domain:
                            visited.append(link)
                            futures.add(executor.submit(crawl_page, link, visited, collected_urls_params, paylaods))
