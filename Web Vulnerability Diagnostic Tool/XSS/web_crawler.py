
# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„í¬íŠ¸í•©ë‹ˆë‹¤.
import requests
from urllib.parse import urljoin, urlparse, parse_qs
from bs4 import BeautifulSoup
from concurrent.futures import ProcessPoolExecutor, wait, FIRST_COMPLETED
from multiprocessing import Manager
from xss_test import test_xss  # xss_test ëª¨ë“ˆì—ì„œ test_xss í•¨ìˆ˜ë¥¼ ì„í¬íŠ¸í•©ë‹ˆë‹¤.

# ê° í˜ì´ì§€ë¥¼ í¬ë¡¤ë§í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.
def crawl_page(url, visited, collected_urls_params, payloads):
    try:
        # í˜ì´ì§€ì— ìš”ì²­ì„ ë³´ëƒ…ë‹ˆë‹¤.
        headers = {"User-Agent": "Mozilla/5.0"}
        for _ in range(3):  # ì¬ì‹œë„ ë¡œì§ì„ ì¶”ê°€í•©ë‹ˆë‹¤.
            try:
                response = requests.get(url, headers=headers)
                break
            except:
                print(f"Retry: {url}")
        else:
            print(f"ğŸŸ¡ Failed to fetch: {url}")
            return url, []

        # BeautifulSoup ê°ì²´ë¥¼ ìƒì„±í•˜ì—¬ HTMLì„ íŒŒì‹±í•©ë‹ˆë‹¤.
        soup = BeautifulSoup(response.text, 'html.parser')

        # ê° í˜ì´ì§€ì˜ <form> íƒœê·¸ ë‚´ë¶€ì˜ <input>, <textarea>, <select>, <button> íƒœê·¸ì˜ name ì†ì„±ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
        for form in soup.find_all('form', method='get'):
            for tag in form.find_all(['input', 'textarea', 'select', 'button']):
                name = tag.get('name')
                if name:
                    param_url = f'{urlparse(url)._replace(query="").geturl()}?{name}='
                    # ìˆ˜ì§‘í•œ URLê³¼ íŒŒë¼ë¯¸í„°ê°€ ì•„ì§ ìˆ˜ì§‘ëœ ì ì´ ì—†ëŠ” ê²½ìš°, ìˆ˜ì§‘ ëª©ë¡ì— ì¶”ê°€í•©ë‹ˆë‹¤.
                    if param_url not in collected_urls_params:
                        collected_urls_params.append(param_url)
                        print(param_url)
                        # XSS í…ŒìŠ¤íŠ¸ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
                        test_xss(param_url, payloads)

        # form ì†ì„±ì„ ê°–ëŠ” <input> íƒœê·¸ì˜ name ì†ì„±ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
        for input_tag in soup.find_all('input', form=True):
            name = input_tag.get('name')
            if name:
                param_url = f'{urlparse(url)._replace(query="").geturl()}?{name}='
                if param_url not in collected_urls_params:
                    collected_urls_params.append(param_url)
                    print(param_url)
                    # XSS í…ŒìŠ¤íŠ¸ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
                    test_xss(param_url, payloads)

        # URLì˜ ì¿¼ë¦¬ íŒŒë¼ë¯¸í„°ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
        parsed_url = urlparse(url)
        params = parse_qs(parsed_url.query)
        for param in params:
            param_url = f'{parsed_url._replace(query="").geturl()}?{param}='
            # ìˆ˜ì§‘í•œ URLê³¼ íŒŒë¼ë¯¸í„°ê°€ ì•„ì§ ìˆ˜ì§‘ëœ ì ì´ ì—†ëŠ” ê²½ìš°, ìˆ˜ì§‘ ëª©ë¡ì— ì¶”ê°€í•©ë‹ˆë‹¤.
            if param_url not in collected_urls_params:
                collected_urls_params.append(param_url)
                print(param_url)
                # XSS í…ŒìŠ¤íŠ¸ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
                test_xss(param_url, payloads)

        # í˜ì´ì§€ ë‚´ì—ì„œ ë°œê²¬í•œ ëª¨ë“  ë§í¬ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
        links = [urljoin(url, link.get('href')) for link in soup.find_all('a') if urljoin(url, link.get('href')) not in visited]

        # í˜„ì¬ í˜ì´ì§€ URLê³¼ ìˆ˜ì§‘í•œ ë§í¬ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        return url, links
    except Exception as e:
        # ì˜¤ë¥˜ê°€ ë°œìƒí•œ ê²½ìš°, ì˜¤ë¥˜ ë©”ì‹œì§€ë¥¼ ì¶œë ¥í•˜ê³  í˜„ì¬ í˜ì´ì§€ URLê³¼ ë¹ˆ ë§í¬ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        print(f"ğŸŸ¡ Error occurred: {e}")
        return url, []

# ì›¹ì‚¬ì´íŠ¸ë¥¼ í¬ë¡¤ë§í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.
def crawl_website(start_url, paylaods):
    with Manager() as manager:
        # ë°©ë¬¸í•œ í˜ì´ì§€ì™€ ìˆ˜ì§‘í•œ URLê³¼ íŒŒë¼ë¯¸í„°ì˜ ìŒì„ ì €ì¥í•˜ëŠ” ë¦¬ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        visited = manager.list([start_url])
        collected_urls_params = manager.list()

        # ì‹œì‘ URLì˜ ë„ë©”ì¸ì„ íŒŒì‹±í•©ë‹ˆë‹¤.
        parsed_start_url = urlparse(start_url)
        start_domain = parsed_start_url.netloc

        # ProcessPoolExecutorë¥¼ ì‚¬ìš©í•˜ì—¬ ë³‘ë ¬ë¡œ í˜ì´ì§€ë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤.
        with ProcessPoolExecutor(max_workers=8) as executor:
            futures = {executor.submit(crawl_page, start_url, visited, collected_urls_params, paylaods)}
            while futures:
                # ì™„ë£Œëœ ì‘ì—…ì„ ê¸°ë‹¤ë¦½ë‹ˆë‹¤.
                done, futures = wait(futures, return_when=FIRST_COMPLETED)
                for future in done:
                    _, links = future.result()
                    for link in links:
                        # ë§í¬ê°€ ì•„ì§ ë°©ë¬¸í•˜ì§€ ì•Šì€ ë§í¬ì´ê³  ê°™ì€ ë„ë©”ì¸ì— ì†í•˜ëŠ” ê²½ìš°, ê·¸ ë§í¬ë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤.
                        if link not in visited and urlparse(link).netloc == start_domain:
                            visited.append(link)
                            futures.add(executor.submit(crawl_page, link, visited, collected_urls_params, paylaods))
